

import os
import tensorflow as tf # å¯¼å…¥TF åº“
from tensorflow import keras # å¯¼å…¥TF å­åº“

from tensorflow_core.python.keras import layers, optimizers, datasets # å¯¼å…¥TF å­åº“

from tensorflow_core.python.keras.optimizers import SGD



# åŠ è½½æ‰‹å†™æ•°å­—å›¾ç‰‡æ•°æ®é›†
# ä¸ºäº†æ–¹ä¾¿ä¸šç•Œç»Ÿä¸€æµ‹è¯•å’Œè¯„ä¼°ç®—æ³•ï¼Œ (Lecun, Bottou, Bengio, & Haffner, 1998)å‘å¸ƒäº†æ‰‹å†™æ•°å­—å›¾
# ç‰‡æ•°æ®é›†ï¼Œå‘½åä¸ºMNISTï¼Œå®ƒåŒ…å«äº†0~9 å…±10 ç§æ•°å­—çš„æ‰‹å†™å›¾ç‰‡ï¼Œæ¯ç§æ•°å­—ä¸€å…±æœ‰7000 å¼ å›¾ç‰‡ï¼Œé‡‡é›†è‡ª
# ä¸åŒä¹¦å†™é£æ ¼çš„çœŸå®æ‰‹å†™å›¾ç‰‡ï¼Œä¸€å…±70000 å¼ å›¾ç‰‡ã€‚å…¶ä¸­60000å¼ å›¾ç‰‡ä½œä¸ºè®­ç»ƒé›†ğ”»train(Training Set)ï¼Œ
# ç”¨æ¥è®­ç»ƒæ¨¡å‹ï¼Œå‰©ä¸‹10000 å¼ å›¾ç‰‡ä½œä¸ºæµ‹è¯•é›†ğ”»test(Test Set)ï¼Œç”¨æ¥é¢„æµ‹æˆ–è€…æµ‹è¯•ï¼Œè®­ç»ƒé›†å’Œæµ‹è¯•é›†å…±
# åŒç»„æˆäº†æ•´ä¸ªMNIST æ•°æ®é›†ã€‚

# åŠ è½½æ•°æ®é›† - Keraså­åº“datasetsè‡ªåŠ¨ä¸‹è½½ã€ç®¡ç†å’ŒåŠ è½½MNIST æ•°æ®é›†
# (x, y)æ˜¯è®­ç»ƒé›†ï¼Œ(x_val, y_val)æ˜¯æµ‹è¯•é›†
(x, y), (x_val, y_val) = datasets.mnist.load_data() 
# load_data()å‡½æ•°è¿”å›ä¸¤ä¸ªå…ƒç»„(tuple)å¯¹è±¡ï¼Œç¬¬ä¸€ä¸ªæ˜¯è®­ç»ƒé›†ï¼Œç¬¬äºŒä¸ªæ˜¯æµ‹è¯•é›†ï¼Œæ¯ä¸ªtupleçš„ç¬¬ä¸€ä¸ªå…ƒç´ 
# æ˜¯å¤šä¸ªè®­ç»ƒå›¾ç‰‡æ•°æ®Xï¼Œç¬¬äºŒä¸ªå…ƒç´ æ˜¯è®­ç»ƒå›¾ç‰‡å¯¹åº”çš„ç±»åˆ«æ•°å­—Yã€‚å…¶ä¸­è®­ç»ƒé›†Xçš„å¤§å°ä¸º(60000,28,28)ï¼Œ
# ä»£è¡¨äº†60000 ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬ç”±28 è¡Œã€28 åˆ—æ„æˆï¼Œç”±äºæ˜¯ç°åº¦å›¾ç‰‡ï¼Œæ•…æ²¡æœ‰RGB é€šé“ï¼›è®­ç»ƒé›†Yçš„å¤§å°
# ä¸º(60000, )ï¼Œä»£è¡¨äº†è¿™60000 ä¸ªæ ·æœ¬çš„æ ‡ç­¾æ•°å­—ï¼Œæ¯ä¸ªæ ·æœ¬æ ‡ç­¾ç”¨ä¸€ä¸ª0~9 çš„æ•°å­—è¡¨ç¤ºã€‚æµ‹è¯•é›†X çš„å¤§å°
# ä¸º(10000,28,28)ï¼Œä»£è¡¨äº†10000 å¼ æµ‹è¯•å›¾ç‰‡ï¼ŒY çš„å¤§å°ä¸º(10000, )ã€‚

# x æ˜¯å¤šä¸ªè®­ç»ƒå›¾ç‰‡æ•°æ®ï¼Œå¤§å°ä¸º(60000,28,28)ï¼Œå³6000ä¸ª28x28çš„ç°åº¦å›¾ 
# è½¬æ¢ä¸ºå¼ é‡ï¼Œå°†0~255ç°åº¦å€¼ç¼©æ”¾åˆ°-1~1
x = 2*tf.convert_to_tensor(x, dtype=tf.float32)/255.-1 

# y æ˜¯è®­ç»ƒæ ·æœ¬çš„æ ‡ç­¾æ•°å­—ï¼Œç”¨ä¸€ä¸ª0~9 çš„æ•°å­—è¡¨ç¤º
y = tf.convert_to_tensor(y, dtype=tf.int32) # è½¬æ¢ä¸ºå¼ é‡
# æ‰‹å†™æ•°å­—æœ‰0~9å…±10ç§åˆ†ç±»ï¼Œå»ºç«‹depth=10çš„one-hotç¼–ç 
y = tf.one_hot(y, depth=10) # one-hot ç¼–ç 


print(x.shape, y.shape)


train_dataset = tf.data.Dataset.from_tensor_slices((x, y)) # æ„å»ºæ•°æ®é›†å¯¹è±¡
train_dataset = train_dataset.batch(512) # æ‰¹é‡è®­ç»ƒ


layers.Dense(256, activation='relu'),
model = keras.Sequential([ # 3 ä¸ªéçº¿æ€§å±‚çš„åµŒå¥—æ¨¡å‹
    layers.Dense(256, activation='relu'),
    layers.Dense(128, activation='relu'),
    layers.Dense(10)])

sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy',
              optimizer=sgd,
              metrics=['accuracy'])

model.fit(x_train, y_train,
          epochs=20,
          batch_size=128)
score = model.evaluate(x_test, y_test, batch_size=128)    


# è‡ªåŠ¨æ±‚å¯¼
with tf.GradientTape() as tape: # æ„å»ºæ¢¯åº¦è®°å½•ç¯å¢ƒ
    # æ‰“å¹³ï¼Œ[b, 28, 28] => [b, 784]
    x = tf.reshape(x, (-1, 28*28))
    # Step1. å¾—åˆ°æ¨¡å‹è¾“å‡ºoutput
    # [b, 784] => [b, 10]
    out = model(x)
    loss = out - y
    # Step3. è®¡ç®—å‚æ•°çš„æ¢¯åº¦ w1, w2, w3, b1, b2, b3
    grads = tape.gradient(loss, x)
    # w' = w - lr * gradï¼Œæ›´æ–°ç½‘ç»œå‚æ•°
    optimizers.apply_gradients(zip(grads, x))